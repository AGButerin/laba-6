import numpy as np

def objective_function(x):
    x1 = x[0]
    x2 = x[1]
    return 7 * x1**2 + 2 * x1 * x2 + 5 * x2**2 + x1 - 10 * x2

def gradient(x):
    x1 = x[0]
    x2 = x[1]
    grad_x1 = 14 * x1 + 2 * x2 + 1
    grad_x2 = 2 * x1 + 10 * x2 - 10
    return np.array([grad_x1, grad_x2])

def steepest_descent(initial_x, tolerance=1e-6, max_iterations=1000):
    x = initial_x
    iteration = 0

    while iteration < max_iterations:
        grad = gradient(x)
        x_new = x - grad

        gradient_norm = np.linalg.norm(grad)

        if gradient_norm < tolerance:
            print(f"Converged to tolerance level. Iteration: {iteration}")
            break

        x = x_new
        iteration += 1

        print(f"Iteration: {iteration}, x1 = {x[0]}, x2 = {x[1]}, Objective function value = {objective_function(x)}")

    print(f"Optimal x1: {x[0]}")
    print(f"Optimal x2: {x[1]}")
    print(f"Optimal objective function value: {objective_function(x)}")

initial_x = np.array([1.0, 1.0])
tolerance = 1e-6
max_iterations = 1000

steepest_descent(initial_x, tolerance, max_iterations)
